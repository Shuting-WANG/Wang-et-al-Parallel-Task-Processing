{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d54f69d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from task_mask import dataset\n",
    "from task_mask import plot_behav_per\n",
    "from task_mask import plot_activity\n",
    "from task_mask import perf_trials\n",
    "from model import Net\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import scipy.io as sio\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def set_seed(seed: int):\n",
    "    \"\"\"Fix Python, NumPy and PyTorch RNGs for reproducibility.\"\"\"\n",
    "    random.seed(seed) # Python\n",
    "    np.random.seed(seed) # NumPy\n",
    "    torch.manual_seed(seed) # Torch\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "def separation_loss(activity, mask, e_size=0.8):\n",
    "    tau         = 0   # target separation: 0 = less correlation, 1 = more correlation\n",
    "    eps         = 1e-6       # small constant\n",
    "    n_exc = int(e_size * activity.shape[2])  # number of excitatory neurons\n",
    "    h_LEVER = activity[mask[:,:,0]==2,:,0:n_exc].mean(dim=0) - activity[mask[:,:,0]==1,:,0:n_exc].mean(dim=0) # activity (Lever window -baseline)\n",
    "    h_GNG = activity[mask[:,:,1]==3,:,0:n_exc].mean(dim=0) - activity[mask[:,:,1]==4,:,0:n_exc].mean(dim=0) # activity (Go - No-Go)\n",
    "    cs = F.cosine_similarity(h_LEVER, h_GNG, dim=0, eps=eps) # cosine similarity\n",
    "    penalty = (tau - cs.abs()).abs() # penalty for deviation from target task seperation\n",
    "    return penalty, h_LEVER, h_GNG\n",
    "\n",
    "seed_list = [0, 42, 1337, 271828, 314159, 1618033, 1414213, 1732051, 2236067, 57721566,\n",
    "             1813382119, 827308000, 1627694679, 1911784258, 903170603, 86939547, 556019486, 2073320062, 1097954098, 1043521779]\n",
    "seed_fail = [0, 314159, 1618033, 1732051, 2236067, 827308000, 1911784258, 86939547, 2073320062, 1043521779] # seed that failed to learn dual-task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "171adbfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = 50\n",
    "input, target, _, _, _, _ = dataset(dt)\n",
    "input_size = input.shape[-1]\n",
    "output_size = target.shape[-1]\n",
    "hidden_size = 100\n",
    "num_trial = 500\n",
    "num_DTT = 100\n",
    "\n",
    "for f, seed in enumerate(seed_list):\n",
    "    set_seed(seed)\n",
    "\n",
    "    if seed in seed_fail:\n",
    "        print(f'Skipping seed {seed} due to known failure.')\n",
    "        continue\n",
    "\n",
    "    net = Net(input_size=input_size, hidden_size=hidden_size, output_size=output_size, dt=dt)\n",
    "    print(net)\n",
    "\n",
    "    # Use Adam optimizer\n",
    "    optimizer = optim.Adam(net.parameters(), lr=0.01, weight_decay=1e-4)\n",
    "    criterion = nn.MSELoss(reduction='sum')\n",
    "    lambda_l1 = 1e-4\n",
    "    lambda_sel  = 1\n",
    "    running_loss = 0\n",
    "\n",
    "    for i in range(1000):\n",
    "        inputs, labels, _, _, _, _ = dataset(dt) \n",
    "        inputs = torch.from_numpy(inputs).float()\n",
    "        labels = torch.from_numpy(labels.flatten()).float()\n",
    "\n",
    "        outputs, activity = net(inputs)\n",
    "        outputs = outputs.view(-1)\n",
    "\n",
    "        loss = criterion(outputs, labels)/(labels != 0).sum() + lambda_l1 * torch.mean(torch.abs(activity))\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "     \n",
    "    activity_dict, performance_dict, trial_infos, task_onsets = {}, {}, {}, {}\n",
    "    for i in range(num_trial):\n",
    "        inputs, labels, _, _, task_onset, trial_type = dataset(dt, batch_size=1, DT=True)\n",
    "        inputs = torch.from_numpy(inputs).float()\n",
    "        labels = torch.from_numpy(labels.flatten()).float()\n",
    "\n",
    "        action_pred, rnn_activity = net(inputs)\n",
    "\n",
    "        performance_dict[i]  = action_pred[:, 0, :].detach().numpy()\n",
    "        activity_dict[i] = rnn_activity[:, 0, :].detach().numpy()\n",
    "        trial_infos[i] = trial_type\n",
    "        task_onsets[i] = task_onset\n",
    "    aligned_performance = plot_behav_per([-1,6],dt,num_trial,trial_infos,performance_dict,task_onsets)\n",
    "    aligned_activity = plot_activity([-1,6],dt,num_trial,trial_infos,activity_dict,task_onsets, sort_task=None)\n",
    "    sio.savemat('runs/20250808_decorrelate_only/run' + str(f) + '_pre.mat', {'aligned_performance': aligned_performance, 'aligned_activity': aligned_activity,\\\n",
    "                                                    'trial_infos': list(trial_infos.values()), 'runing_loss': running_loss})\n",
    "\n",
    "    # dual-task learning\n",
    "    # Freeze the input and output layer parameters\n",
    "    for param in net.rnn.input2h.parameters():\n",
    "        param.requires_grad = False\n",
    "    for param in net.rnn.h2h.parameters():\n",
    "        param.requires_grad = True\n",
    "    for param in net.fc.parameters():\n",
    "        param.requires_grad = False\n",
    "    # Create an optimiser that only updates parameters with requires_grad=True\n",
    "    optimizer = optim.Adam(filter(lambda p: p.requires_grad, net.parameters()), lr=0.01, weight_decay=1e-4)\n",
    "\n",
    "    # index for updating using loss 1 (to improve task performance); the rest using loss 2 (to improve task performance and to separate)\n",
    "    # extract index for dual-task trials\n",
    "    idx_GO    = [k for k, v in trial_infos.items() if v[0] == 4]  # index for dual-task Go trials\n",
    "    idx_NG    = [k for k, v in trial_infos.items() if v[0] == 5]  # index for dual-task No-Go trials\n",
    "    # extract motor task and cognitive task activity\n",
    "    aligned_activity = np.array(aligned_activity)  # shape: (num_trial, time_steps, n_neurons)\n",
    "    activity_base    = aligned_activity[idx_NG, 0:20, :].mean(axis=0).mean(axis=0) # only use DT-NG trial activity for calculating CD_motor; baseline window: 0-20\n",
    "    activity_LEVER = aligned_activity[idx_NG, 20:120, :].mean(axis=0).mean(axis=0) # only use DT-NG trial activity for calculating CD_motor; lever window: 20-120\n",
    "    activity_GO    =  aligned_activity[idx_GO, 60:84, :].mean(axis=0).mean(axis=0) # Go cue + response window: 60-84\n",
    "    activity_NG    =  aligned_activity[idx_NG, 60:84, :].mean(axis=0).mean(axis=0) # No-Go cue + response window: 60-84\n",
    "    # find neurons that are negatively modulated by the motor task and the cognitive task: task coordination mediated by sacrificing one task for the other\n",
    "    freeze_idx = np.nonzero(((activity_LEVER - activity_base) * (activity_GO - activity_NG)) < 0)[0]\n",
    "    freeze_idx_tensor = torch.tensor(freeze_idx, dtype=torch.long)\n",
    "    mask1 = torch.zeros(hidden_size)\n",
    "    mask1[freeze_idx_tensor] = 1.0\n",
    "    mask2 = 1.0 - mask1\n",
    "    mask1 = mask1.to(net.rnn.h2h.weight.device).view(-1, 1)\n",
    "    mask2 = mask2.to(net.rnn.h2h.weight.device).view(-1, 1)\n",
    "\n",
    "    def mask_grad_hook(mask):\n",
    "        def hook(grad):\n",
    "            return grad * mask\n",
    "        return hook\n",
    "\n",
    "    running_loss = 0\n",
    "    st_motorSuc_per, st_hit_per, st_cr_per = np.zeros(num_DTT), np.zeros(num_DTT), np.zeros(num_DTT)\n",
    "    dt_motorSuc_per, dt_hit_per, dt_cr_per = np.zeros(num_DTT), np.zeros(num_DTT), np.zeros(num_DTT)\n",
    "    for i in range(num_DTT):\n",
    "        while True:\n",
    "            inputs, labels, CD_mask, dual_motor_mask, task_onset, trial_type = dataset(dt, DT=True)\n",
    "            inputs = torch.from_numpy(inputs).float()\n",
    "            labels = torch.from_numpy(labels.flatten()).float()\n",
    "            dual_motor_mask = torch.from_numpy(dual_motor_mask.flatten()).float()\n",
    "            task_onset = torch.from_numpy(np.array(task_onset)).float()\n",
    "            trial_type = torch.from_numpy(np.array(trial_type)).float()\n",
    "            if trial_type.unique().numel() >= 5:\n",
    "                break\n",
    "\n",
    "        outputs, activity = net(inputs)\n",
    "        outputs = outputs.view(-1)\n",
    "        \n",
    "        # calculate task performance for each step\n",
    "        st_motorSuc_per[i], st_hit_per[i], st_cr_per[i], dt_motorSuc_per[i], dt_hit_per[i], dt_cr_per[i] = \\\n",
    "            perf_trials(outputs, task_onset, trial_type, dt)\n",
    "        \n",
    "        # Backward with gradient masking\n",
    "        # motor performance outside dual-task phase and all cognitive performance\n",
    "        loss_motor_outDT_cog = criterion(outputs[dual_motor_mask==0], labels[dual_motor_mask==0])  \n",
    "        # within dual-task phase, only calculate the loss if the output is less than 0.3\n",
    "        loss_motor_inDT = criterion(outputs[dual_motor_mask==1][outputs[dual_motor_mask==1]<0.3], labels[dual_motor_mask==1][outputs[dual_motor_mask==1]<0.3])\n",
    "        loss1 = (loss_motor_outDT_cog + loss_motor_inDT)/(labels != 0).sum() + lambda_l1 * activity.abs().mean()\n",
    "        loss2 = loss1 + lambda_sel*separation_loss(activity, CD_mask)[0]\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        # loss 1: focus on task performance\n",
    "        handle = net.rnn.h2h.weight.register_hook(mask_grad_hook(mask1))\n",
    "        loss2.backward(retain_graph=True)\n",
    "        handle.remove()\n",
    "        # loss 2: focus on task performance and task separation\n",
    "        handle = net.rnn.h2h.weight.register_hook(mask_grad_hook(mask2))\n",
    "        loss2.backward()\n",
    "        handle.remove()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss1.item()\n",
    "        print(f'Iteration {i+1}, Loss: {loss1:.4f}')\n",
    "        print(f'separation_loss: {lambda_sel * separation_loss(activity, CD_mask)[0]:.4f}')\n",
    "\n",
    "\n",
    "    activity_dict, performance_dict, trial_infos, task_onsets = {}, {}, {}, {}\n",
    "    for i in range(num_trial):\n",
    "        inputs, labels, _, _, task_onset, trial_type = dataset(dt, batch_size=1, DT=True)\n",
    "        inputs = torch.from_numpy(inputs).float()\n",
    "        labels = torch.from_numpy(labels.flatten()).float()\n",
    "\n",
    "        action_pred, rnn_activity = net(inputs)\n",
    "        performance_dict[i] = action_pred[:, 0, :].detach().numpy()\n",
    "        activity_dict[i] = rnn_activity[:, 0, :].detach().numpy()\n",
    "\n",
    "        trial_infos[i] = trial_type\n",
    "        task_onsets[i] = task_onset\n",
    "    aligned_performance = plot_behav_per([-1,6],dt,num_trial,trial_infos,performance_dict,task_onsets)\n",
    "    aligned_activity = plot_activity([-1,6],dt,num_trial,trial_infos,activity_dict,task_onsets, sort_task=None)\n",
    "\n",
    "    sio.savemat('runs/20250808_decorrelate_only/run' + str(f) + '_post.mat', {'aligned_performance': aligned_performance, 'aligned_activity': aligned_activity, \\\n",
    "                                                    'trial_infos': list(trial_infos.values()), 'runing_loss': running_loss, \\\n",
    "                                                    'st_motorSuc_per': st_motorSuc_per, 'st_hit_per': st_hit_per, 'st_cr_per': st_cr_per, \\\n",
    "                                                    'dt_motorSuc_per': dt_motorSuc_per, 'dt_hit_per': dt_hit_per, 'dt_cr_per': dt_cr_per})\n",
    "\n",
    "    # check task separation\n",
    "    _, h_LEVER, h_GNG =  separation_loss(activity, CD_mask) \n",
    "    plt.figure(figsize=(5, 5))\n",
    "    plt.scatter(h_LEVER.detach().numpy(), h_GNG.detach().numpy(), c='blue')\n",
    "    plt.xlabel('activity in motor task')\n",
    "    plt.ylabel('activity in cognitive task')\n",
    "    plt.show()\n",
    "    print(F.cosine_similarity(h_LEVER, h_GNG, dim=0, eps=1e-6))\n",
    "\n",
    "    # check performance\n",
    "    fig, ax = plt.subplots(1, 2, figsize=(6, 3), sharex=True, sharey=True)\n",
    "    ax[0].plot(st_motorSuc_per, label='st_motorSuc')\n",
    "    ax[0].plot(dt_motorSuc_per, label='dt_motorSuc')\n",
    "    ax[0].set_ylabel('motor task performance')\n",
    "    ax[1].plot(st_hit_per+st_cr_per, label='st_GNG')\n",
    "    ax[1].plot(dt_hit_per+dt_cr_per, label='dt_GNG')\n",
    "    ax[1].set_ylabel('cognitive task performance')\n",
    "    plt.xlim([0,num_DTT])\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ds",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
